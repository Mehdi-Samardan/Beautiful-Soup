import requests
from bs4 import BeautifulSoup

# ğŸŒŸ KullanÄ±cÄ± bilgilerini tarayÄ±cÄ± gibi gÃ¶stermek iÃ§in sahte User-Agent
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}

# ğŸŒŸ TarayÄ±cÄ±dan alÄ±nan Ã§erezler (Gerekiyorsa gÃ¼ncelle)
COOKIES = {
    # "cookie_name": "cookie_value"  # Ã‡erez kullanmÄ±yorsan bunu boÅŸ bÄ±rak
}

# ğŸŒŸ Proxy kullanmÄ±yorsan burayÄ± tamamen kaldÄ±r
# PROXIES = {
#     "http": "http://your_proxy:port",
#     "https": "https://your_proxy:port"
# }

# ğŸŒŸ Web sayfasÄ±nÄ± kazÄ±yan fonksiyon
def scrape_and_save(url, filename):
    session = requests.Session()
    session.headers.update(HEADERS)  # Headers gÃ¼ncelle
    session.cookies.update(COOKIES)  # Ã‡erezleri ekle (varsa)

    try:
        # EÄŸer proxy kullanmÄ±yorsan bu satÄ±rda 'proxies=PROXIES' KALDIRILDI!
        response = session.get(url)  

        if response.status_code == 403:
            print("ğŸš« 403 HatasÄ±: EriÅŸim engellendi. TarayÄ±cÄ± bilgilerini gÃ¼ncelleyerek tekrar dene!")
            return
        elif response.status_code != 200:
            print(f"âš ï¸ Hata: Sayfa {response.status_code} ile yanÄ±t verdi.")
            return

        soup = BeautifulSoup(response.text, 'html.parser')

        # ğŸ† Sayfa baÅŸlÄ±ÄŸÄ± (H1)
        title = soup.find('h1').text.strip() if soup.find('h1') else "BaÅŸlÄ±k bulunamadÄ±"

        # ğŸ“œ Temel iÃ§erik (p, a, ul, ol, li)
        content_tags = soup.find_all(['p', 'a', 'ul', 'ol', 'li'])
        content = "\n".join(tag.get_text(strip=True) for tag in content_tags)

        # ğŸ–¼ï¸ Medya iÃ§erikleri (img kaynaklarÄ±)
        media = [img['src'] for img in soup.find_all('img') if 'src' in img.attrs]

        # ğŸ’¾ Verileri dosyaya kaydetme
        with open(filename, 'w', encoding='utf-8') as file:
            file.write(f"BaÅŸlÄ±k: {title}\n\n")
            file.write("Ä°Ã§erik:\n")
            file.write(content + "\n\n")
            file.write("Medya:\n")
            for src in media:
                file.write(src + "\n")

        print(f"âœ… Veriler '{filename}' dosyasÄ±na baÅŸarÄ±yla kaydedildi!")

    except requests.exceptions.RequestException as e:
        print(f"â›” Ä°stek hatasÄ±: {e}")

# ğŸŒ KullanÄ±lacak URL ve dosya adÄ±
URL = "https://www.ellindecoratie.nl/boomstam-bijzettafeltje"
FILENAME = "ellindecoratie_verileri.txt"

# ğŸš€ Web sitesini kazÄ± ve verileri dosyaya kaydet
scrape_and_save(URL, FILENAME)
